---
title: "Lab 9 - HPC"
output: 
html_document: default
link-citations: yes
---

# Learning goals

In this lab, you are expected to practice the following skills:

- Evaluate whether a problem can be parallelized or not.
- Practice with the parallel package.
- Use Rscript to submit jobs.

```{r eval=FALSE, echo=FALSE}
# install any missing packages
install.packages("microbenchmark")
```

## Problem 1

Give yourself a few minutes to think about what you learned about parallelization. List three
examples of problems that you believe may be solved using parallel computing,
and check for packages on the HPC CRAN task view that may be related to it.

1. Cross Validation in Machine Learning:
- caret: Supports parallel cross-validation for model training.
- mlr: Provides tools for machine learning and supports parallel model training.
- foreach: Enables parallel execution for loops in R, often used for model training.
- doParallel: Works with foreach to allow parallel execution on multiple processors.

2. Boostrapping
- boot: Bootstrapping is a resampling technique used to estimate the distribution of a statistic. The boot package implements this method, and it can be made more efficient by using the parallel package to handle multiple resampling operations at once.

3. Markov Chain Monte Carlo (MCMC):
- parallel: Provides general parallel computing capabilities in R.
- rstan: Interface to Stan for Bayesian modeling, with parallel support.
- RcppParallel: Enables parallel MCMC sampling in R.
- nimble: Customizes Bayesian inference and supports parallel MCMC sampling

## Problem 2: Pre-parallelization

The following functions can be written to be more efficient without using
`parallel`:

1. This function generates a `n x k` dataset with all its entries having a Poisson distribution with mean `lambda`.

```{r p2-fun1}
fun1 <- function(n = 100, k = 4, lambda = 4) {
  x <- NULL
  
  for (i in 1:n)
    x <- rbind(x, rpois(k, lambda))
  
  return(x)
}

fun1alt <- function(n = 100, k = 4, lambda = 4) {
  matrix(rpois(n * k, lambda = lambda), ncol = k)
}

# Benchmarking
microbenchmark::microbenchmark(
  fun1(100),
  fun1alt(100),
  unit="ns"
)
```

How much faster?

_The fun1alt(100) is roughly 9 times faster than fun1(100) in terms of the minimum time, and even though both have similar maximum times, fun1alt still outperforms fun1 due to the more efficient matrix construction in fun1alt_


2.  Find the column max (hint: Checkout the function `max.col()`).

```{r p2-fun2}
# Data Generating Process (10 x 10,000 matrix)
set.seed(1234)
x <- matrix(rnorm(1e4), nrow=10)

# Find each column's max value
fun2 <- function(x) {
  apply(x, 2, max)
}

fun2alt <- function(x) {
  # YOUR CODE HERE
  x[cbind(max.col(t(x)), 1:ncol(x))]
  
  # avoid function calls inside loops
  # directly extracting the max values, without any loops
}

# Benchmarking
bench <- microbenchmark::microbenchmark(
  fun2(x),
  fun2alt(x),
  unit="us"
)
```

_Answer here with a plot._


```{r}
plot(bench)
ggplot2::autoplot(bench) +
  ggplot2::theme_minimal()
```

## Problem 3: Parallelize everything

We will now turn our attention to non-parametric 
[bootstrapping](https://en.wikipedia.org/wiki/Bootstrapping_(statistics)).
Among its many uses, non-parametric bootstrapping allow us to obtain confidence
intervals for parameter estimates without relying on parametric assumptions.

The main assumption is that we can approximate many experiments by resampling
observations from our original dataset, which reflects the population. 

This function implements the non-parametric bootstrap:

```{r p3-boot-fun, eval = FALSE}
my_boot <- function(dat, stat, R, ncpus = 1L) {
  
  # Getting the random indices
  n <- nrow(dat)
  idx <- matrix(sample.int(n, n*R, TRUE), nrow=n, ncol=R)
 
  # Making the cluster using `ncpus`
  # STEP 1: Creating cluster for parallel computing
  cl <- makePSOCKcluster(ncpus) # ncpus specifying using multiple CPU cores
  # PSCOK (Parallel Socket Cluster)
  
  # STEP 2: Prevent memory leak and export the variables to the cluster
  # on.exit(stopCluster(cl))
  clusterExport(cl, varlist = c("idx", "dat", "stat"), envir = environment())
  
  # sending the variables to all worker nodes
  # each run in isolated environment, dont have access to global variable
  # idx -> resampling indices for boostrapping
  # dat -> dataset
  # stat -> statistical function that we use to compute the estimates
  
  # change sequential apply to parallelized apply
  # STEP 3: THIS FUNCTION NEEDS TO BE REPLACED WITH parLapply
  ans <- lapply(seq_len(R), function(i) {
    stat(dat[idx[,i], , drop=FALSE])
  })
  
  # Coercing the list into a matrix
  ans <- do.call(rbind, ans)
  
  # STEP 4: Free up the system resources
  stopCluster(cl)
  
  ans
  
}
```

1. Use the previous pseudocode, and make it work with `parallel`. Here is just an example for you to try:

```{r p3-test-boot, eval = FALSE}
# Bootstrap of a linear regression model
library(parallel)
my_stat <- function(d) coef(lm(y ~ x, data = d))

# DATA SIM
set.seed(1)
n <- 500 
R <- 1e4
x <- cbind(rnorm(n)) 
y <- x*5 + rnorm(n)

# Check if we get something similar as lm
# OLS Confidence Interval
ans0 <- confint(lm(y ~ x))
cat("OLS CI")
ans0

ans1 <- my_boot(dat = data.frame(x, y), my_stat, R = R, ncpus = 4)
qs <- c(.025, .975)
cat("Boostrap CI\n")
print(t(apply(ans1, 2, quantile, probs = qs)))
```

2. Check whether your version actually goes faster than the non-parallel version:

```{r benchmark-problem3, eval = FALSE}
parallel::detectCores()

# non-parallel 1 core
system.time(my_boot(dat = data.frame(x, y), my_stat, R = 4000, ncpus = 1L))

# parallel 4 core
system.time(my_boot(dat = data.frame(x, y), my_stat, R = 4000, ncpus = 4L))
```

_The 4 cores should run faster than the 1 core_

## Problem 4: Compile this markdown document using Rscript

Once you have saved this Rmd file, try running the following command
in your terminal:

```bash
Rscript --vanilla -e 'rmarkdown::render("[full-path-to-your-Rmd-file.Rmd]")' &
```

Where `[full-path-to-your-Rmd-file.Rmd]` should be replace with the full path to
your Rmd file... :).


